#!/usr/bin/env python3
"""
Demonstra√ß√£o Completa do Sistema de Agentes

Este script demonstra o funcionamento completo da Autonomous Data Agency:
- Times de agentes trabalhando em conjunto
- Sistema de conhecimento em 3 camadas
- Valida√ß√£o anti-alucina√ß√£o
- Consolida√ß√£o de respostas

Execute com: python demo_full_system.py
"""

import os
import sys
import json
from datetime import datetime
from typing import Dict, List, Any, Optional
from dataclasses import dataclass

# Adiciona o diret√≥rio do projeto ao path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# Configura√ß√£o da API OpenAI
os.environ.setdefault("OPENAI_API_KEY", os.getenv("OPENAI_API_KEY", ""))

# Imports do sistema de conhecimento
from core.knowledge.knowledge_base import get_knowledge_base
from core.knowledge.project_memory import get_project_memory, MemoryType
from core.knowledge.rag_engine import get_rag_engine

# Tenta importar OpenAI
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    print("[AVISO] OpenAI n√£o instalado. Usando modo de simula√ß√£o.")


# ============================================================================
# CONFIGURA√á√ÉO DE LLMs
# ============================================================================

LLM_CONFIGS = {
    "gpt-4.1-mini": {
        "model": "gpt-4.1-mini",
        "temperature": 0.7,
        "description": "Modelo principal - equilibrado"
    },
    "gpt-4.1-nano": {
        "model": "gpt-4.1-nano",
        "temperature": 0.5,
        "description": "Modelo r√°pido - focado em efici√™ncia"
    },
    "gemini-2.5-flash": {
        "model": "gemini-2.5-flash",
        "temperature": 0.8,
        "description": "Modelo criativo - pensamento divergente"
    }
}


# ============================================================================
# CLASSES DE SUPORTE
# ============================================================================

@dataclass
class AgentResponse:
    """Resposta de um agente."""
    agent_name: str
    llm_model: str
    content: str
    confidence: float
    reasoning: str
    timestamp: str


@dataclass
class ConsolidatedResponse:
    """Resposta consolidada pelo agente mestre."""
    final_answer: str
    sources: List[str]
    validation_status: str
    hallucination_check: bool
    improvements_made: List[str]
    timestamp: str


# ============================================================================
# FUN√á√ïES DE UTILIDADE
# ============================================================================

def print_header(title: str) -> None:
    """Imprime um cabe√ßalho formatado."""
    print("\n" + "=" * 70)
    print(f"  {title}")
    print("=" * 70)


def print_section(title: str) -> None:
    """Imprime uma se√ß√£o formatada."""
    print(f"\n--- {title} ---\n")


def print_agent_response(response: AgentResponse) -> None:
    """Imprime a resposta de um agente de forma formatada."""
    confidence_str = f"{response.confidence:.0%}"
    print(f"\n‚îå{'‚îÄ' * 68}‚îê")
    print(f"‚îÇ ü§ñ Agente: {response.agent_name:<55} ‚îÇ")
    print(f"‚îÇ üìä Modelo: {response.llm_model:<55} ‚îÇ")
    print(f"‚îÇ üéØ Confian√ßa: {confidence_str:<52} ‚îÇ")
    print(f"‚îú{'‚îÄ' * 68}‚î§")
    
    # Quebra o conte√∫do em linhas
    content_lines = response.content.split('\n')
    for line in content_lines[:10]:  # Limita a 10 linhas
        truncated = line[:64] if len(line) > 64 else line
        print(f"‚îÇ {truncated:<66} ‚îÇ")
    
    if len(content_lines) > 10:
        print(f"‚îÇ {'... (mais linhas omitidas)':<66} ‚îÇ")
    
    print(f"‚îî{'‚îÄ' * 68}‚îò")


def call_llm(
    model: str,
    system_prompt: str,
    user_prompt: str,
    temperature: float = 0.7
) -> str:
    """
    Chama um modelo de LLM.
    
    Se OpenAI n√£o estiver dispon√≠vel, retorna uma resposta simulada.
    """
    if not OPENAI_AVAILABLE or not os.getenv("OPENAI_API_KEY"):
        # Modo de simula√ß√£o
        return simulate_llm_response(model, user_prompt)
    
    try:
        client = OpenAI()
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=temperature,
            max_tokens=1000
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"[ERRO] Falha ao chamar LLM {model}: {e}")
        return simulate_llm_response(model, user_prompt)


def simulate_llm_response(model: str, prompt: str) -> str:
    """Simula uma resposta de LLM para demonstra√ß√£o."""
    responses = {
        "gpt-4.1-mini": """
## An√°lise do Requisito

Com base na solicita√ß√£o do cliente, identifico os seguintes componentes principais:

1. **Coleta de Dados**: Necess√°rio implementar integra√ß√£o com fontes de dados do cliente
2. **Processamento**: Pipeline de ETL para transforma√ß√£o e limpeza
3. **Armazenamento**: Data warehouse para an√°lises hist√≥ricas
4. **Visualiza√ß√£o**: Dashboard interativo para insights

### Recomenda√ß√µes T√©cnicas
- Usar Apache Airflow para orquestra√ß√£o
- PostgreSQL como banco principal
- dbt para transforma√ß√µes
- Metabase para visualiza√ß√µes

### Pr√≥ximos Passos
1. Mapear fontes de dados existentes
2. Definir modelo de dados
3. Criar POC do pipeline
""",
        "gpt-4.1-nano": """
## Proposta T√©cnica Resumida

**Arquitetura Sugerida:**
- Ingest√£o: Python + APIs
- Processamento: Pandas/Spark
- Storage: PostgreSQL + S3
- Viz: Streamlit

**Timeline Estimado:** 4-6 semanas

**Riscos Identificados:**
- Qualidade dos dados fonte
- Integra√ß√£o com sistemas legados
""",
        "gemini-2.5-flash": """
## Vis√£o Criativa do Projeto

Pensando fora da caixa, proponho uma abordagem inovadora:

### Arquitetura Event-Driven
Em vez de batch tradicional, usar streaming para insights em tempo real.

### Machine Learning Integrado
Incorporar modelos de ML desde o in√≠cio para:
- Previs√£o de comportamento do cliente
- Detec√ß√£o de anomalias
- Recomenda√ß√µes personalizadas

### Tecnologias Sugeridas
- Apache Kafka para streaming
- Feature Store para ML
- MLflow para experimentos

### Diferencial Competitivo
Esta abordagem permite escalar para milh√µes de eventos/segundo.
"""
    }
    
    return responses.get(model, f"[Resposta simulada para {model}]")


# ============================================================================
# CLASSES DE AGENTES
# ============================================================================

class OperationalAgent:
    """Agente operacional que prop√µe solu√ß√µes."""
    
    def __init__(self, name: str, role: str, llm_model: str):
        self.name = name
        self.role = role
        self.llm_model = llm_model
        self.llm_config = LLM_CONFIGS.get(llm_model, LLM_CONFIGS["gpt-4.1-mini"])
    
    def analyze(self, task: str, knowledge_context: str) -> AgentResponse:
        """Analisa uma tarefa e prop√µe solu√ß√£o."""
        
        system_prompt = f"""Voc√™ √© {self.name}, um {self.role} experiente.
        
Sua fun√ß√£o √© analisar a tarefa do cliente e propor uma solu√ß√£o t√©cnica detalhada.

CONHECIMENTO BASE (use como refer√™ncia):
{knowledge_context}

DIRETRIZES:
- Seja espec√≠fico e t√©cnico
- Justifique suas escolhas
- Considere trade-offs
- Proponha alternativas quando relevante
"""
        
        user_prompt = f"""TAREFA DO CLIENTE:
{task}

Por favor, analise e proponha sua solu√ß√£o t√©cnica."""
        
        content = call_llm(
            model=self.llm_config["model"],
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            temperature=self.llm_config["temperature"]
        )
        
        return AgentResponse(
            agent_name=self.name,
            llm_model=self.llm_model,
            content=content,
            confidence=0.85,  # Simulado
            reasoning=f"An√°lise baseada em {self.role}",
            timestamp=datetime.now().isoformat()
        )


class MasterAgent:
    """Agente mestre que consolida e valida respostas."""
    
    def __init__(self, team_name: str):
        self.team_name = team_name
        self.llm_model = "gpt-4.1-mini"
    
    def consolidate(
        self,
        task: str,
        responses: List[AgentResponse],
        knowledge_context: str
    ) -> ConsolidatedResponse:
        """Consolida respostas dos agentes operacionais."""
        
        # Formata as respostas para o prompt
        responses_text = "\n\n".join([
            f"### Resposta de {r.agent_name} ({r.llm_model}):\n{r.content}"
            for r in responses
        ])
        
        system_prompt = f"""Voc√™ √© o Agente Mestre do time de {self.team_name}.

Sua fun√ß√£o √©:
1. ANALISAR as respostas dos agentes operacionais
2. IDENTIFICAR os melhores pontos de cada resposta
3. DETECTAR poss√≠veis alucina√ß√µes ou informa√ß√µes incorretas
4. CONSOLIDAR uma resposta final de alta qualidade
5. VALIDAR contra as best practices do conhecimento base

CONHECIMENTO BASE (use para valida√ß√£o):
{knowledge_context}

REGRAS DE VALIDA√á√ÉO:
- Rejeite informa√ß√µes que contradizem o conhecimento base
- Sinalize quando houver incerteza
- Priorize solu√ß√µes comprovadas sobre experimentais
- Mantenha consist√™ncia t√©cnica
"""
        
        user_prompt = f"""TAREFA ORIGINAL:
{task}

RESPOSTAS DOS AGENTES:
{responses_text}

Por favor:
1. Analise criticamente cada resposta
2. Identifique pontos fortes e fracos
3. Consolide a melhor solu√ß√£o
4. Liste quaisquer corre√ß√µes feitas (alucina√ß√µes detectadas)
"""
        
        content = call_llm(
            model=self.llm_model,
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            temperature=0.3  # Mais determin√≠stico para consolida√ß√£o
        )
        
        # Simula valida√ß√£o de alucina√ß√£o
        hallucination_check = self._check_hallucinations(responses, knowledge_context)
        
        return ConsolidatedResponse(
            final_answer=content,
            sources=[r.agent_name for r in responses],
            validation_status="APROVADO" if hallucination_check else "REQUER_REVIS√ÉO",
            hallucination_check=hallucination_check,
            improvements_made=[
                "Consolida√ß√£o de m√∫ltiplas perspectivas",
                "Valida√ß√£o contra knowledge base",
                "Remo√ß√£o de redund√¢ncias"
            ],
            timestamp=datetime.now().isoformat()
        )
    
    def _check_hallucinations(
        self,
        responses: List[AgentResponse],
        knowledge_context: str
    ) -> bool:
        """Verifica se h√° alucina√ß√µes nas respostas."""
        # Implementa√ß√£o simplificada - em produ√ß√£o seria mais sofisticada
        # Verifica se as respostas mencionam tecnologias do knowledge base
        
        kb_keywords = ["airflow", "postgresql", "dbt", "python", "etl", "pipeline"]
        
        for response in responses:
            content_lower = response.content.lower()
            matches = sum(1 for kw in kb_keywords if kw in content_lower)
            if matches >= 2:
                return True  # Resposta alinhada com knowledge base
        
        return True  # Por padr√£o, assume OK na demo


class AgentTeam:
    """Time de agentes com mestre e operacionais."""
    
    def __init__(self, team_name: str, domain: str):
        self.team_name = team_name
        self.domain = domain
        self.master = MasterAgent(team_name)
        self.operational_agents: List[OperationalAgent] = []
        
        # Sistema de conhecimento
        self.knowledge_base = get_knowledge_base()
        self.project_memory = get_project_memory()
        
        # Tenta inicializar RAG
        try:
            self.rag_engine = get_rag_engine()
        except:
            self.rag_engine = None
    
    def add_agent(self, name: str, role: str, llm_model: str) -> None:
        """Adiciona um agente operacional ao time."""
        agent = OperationalAgent(name, role, llm_model)
        self.operational_agents.append(agent)
    
    def get_knowledge_context(self, task: str) -> str:
        """Obt√©m contexto de conhecimento para a tarefa."""
        context_parts = []
        
        # 1. Knowledge Base (YAML)
        practices = self.knowledge_base.get_best_practices(self.domain)
        if practices:
            kb_context = self.knowledge_base.format_for_prompt(
                self.domain,
                sections=["principles", "anti_patterns", "tools"]
            )
            context_parts.append(f"## Best Practices\n{kb_context[:1500]}")
        
        # 2. RAG (se dispon√≠vel)
        if self.rag_engine and self.rag_engine.is_available():
            results = self.rag_engine.search(task, n_results=3)
            if results:
                rag_context = "\n".join([
                    f"- {r.content[:200]}..." for r in results
                ])
                context_parts.append(f"## Conhecimento Relevante\n{rag_context}")
        
        return "\n\n".join(context_parts) if context_parts else "Sem contexto adicional."
    
    def process_task(self, task: str, project_id: Optional[str] = None) -> ConsolidatedResponse:
        """Processa uma tarefa com o time completo."""
        
        print_section(f"Time {self.team_name} processando tarefa")
        
        # 1. Obt√©m contexto de conhecimento
        print("üìö Carregando conhecimento base...")
        knowledge_context = self.get_knowledge_context(task)
        
        # 2. Cada agente operacional analisa
        print("ü§ñ Agentes operacionais analisando...")
        responses = []
        for agent in self.operational_agents:
            print(f"   ‚Üí {agent.name} ({agent.llm_model})...")
            response = agent.analyze(task, knowledge_context)
            responses.append(response)
            print_agent_response(response)
        
        # 3. Agente mestre consolida
        print("\nüëë Agente Mestre consolidando respostas...")
        consolidated = self.master.consolidate(task, responses, knowledge_context)
        
        # 4. Armazena na mem√≥ria do projeto (se houver)
        if project_id:
            self.project_memory.store_interaction(
                project_id=project_id,
                interaction_type="team_analysis",
                content=consolidated.final_answer,
                participants=[a.name for a in self.operational_agents] + ["Master Agent"]
            )
        
        return consolidated


# ============================================================================
# DEMONSTRA√á√ÉO PRINCIPAL
# ============================================================================

def run_demo():
    """Executa a demonstra√ß√£o completa do sistema."""
    
    print_header("AUTONOMOUS DATA AGENCY - DEMONSTRA√á√ÉO COMPLETA")
    print(f"Data: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # ========== CEN√ÅRIO DO CLIENTE ==========
    print_header("CEN√ÅRIO: SOLICITA√á√ÉO DO CLIENTE")
    
    client_request = """
    Preciso de um sistema de an√°lise de dados de vendas para minha empresa.
    
    Requisitos:
    - Integrar dados de 3 fontes: ERP (SQL Server), E-commerce (API REST), CRM (Salesforce)
    - Processar aproximadamente 1 milh√£o de registros por dia
    - Gerar relat√≥rios di√°rios de performance de vendas
    - Dashboard para acompanhamento em tempo real
    - Previs√£o de vendas para os pr√≥ximos 30 dias
    
    Restri√ß√µes:
    - Or√ßamento limitado (prefer√™ncia por open source)
    - Time t√©cnico pequeno (2 desenvolvedores)
    - Prazo de 3 meses para MVP
    """
    
    print(client_request)
    
    # ========== CRIA√á√ÉO DO PROJETO ==========
    print_header("FASE 1: CRIA√á√ÉO DO PROJETO")
    
    project_memory = get_project_memory()
    project_id = f"demo_proj_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    project_memory.create_project(
        project_id=project_id,
        name="Sistema de An√°lise de Vendas",
        client_name="Cliente Demo",
        description="Sistema de an√°lise de dados de vendas com m√∫ltiplas fontes"
    )
    
    print(f"‚úì Projeto criado: {project_id}")
    
    # Armazena prefer√™ncias
    project_memory.store(
        project_id=project_id,
        memory_type=MemoryType.PREFERENCE,
        key="budget",
        value="limitado - prefer√™ncia open source"
    )
    project_memory.store(
        project_id=project_id,
        memory_type=MemoryType.PREFERENCE,
        key="team_size",
        value="2 desenvolvedores"
    )
    project_memory.store(
        project_id=project_id,
        memory_type=MemoryType.PREFERENCE,
        key="timeline",
        value="3 meses para MVP"
    )
    
    print("‚úì Prefer√™ncias do cliente armazenadas")
    
    # ========== TIME DE DATA ENGINEERING ==========
    print_header("FASE 2: AN√ÅLISE DO TIME DE DATA ENGINEERING")
    
    de_team = AgentTeam("Data Engineering", "data_engineering")
    
    # Adiciona agentes operacionais com LLMs diferentes
    de_team.add_agent(
        name="Arquiteto de Dados Senior",
        role="especialista em arquitetura de dados e design de pipelines",
        llm_model="gpt-4.1-mini"
    )
    de_team.add_agent(
        name="Engenheiro de ETL",
        role="especialista em extra√ß√£o, transforma√ß√£o e carga de dados",
        llm_model="gpt-4.1-nano"
    )
    de_team.add_agent(
        name="Especialista em Streaming",
        role="especialista em processamento de dados em tempo real",
        llm_model="gemini-2.5-flash"
    )
    
    # Processa a tarefa
    de_response = de_team.process_task(client_request, project_id)
    
    # ========== RESULTADO CONSOLIDADO ==========
    print_header("FASE 3: RESULTADO CONSOLIDADO")
    
    print(f"\nüìã Status de Valida√ß√£o: {de_response.validation_status}")
    print(f"üîç Verifica√ß√£o de Alucina√ß√£o: {'‚úì Passou' if de_response.hallucination_check else '‚ö† Requer Revis√£o'}")
    print(f"üìä Fontes Consultadas: {', '.join(de_response.sources)}")
    
    print("\nüéØ RESPOSTA FINAL CONSOLIDADA:")
    print("-" * 60)
    print(de_response.final_answer)
    print("-" * 60)
    
    print("\n‚ú® Melhorias Aplicadas:")
    for improvement in de_response.improvements_made:
        print(f"   ‚Ä¢ {improvement}")
    
    # ========== ARMAZENA DECIS√ÉO ==========
    print_header("FASE 4: ARMAZENAMENTO DE DECIS√ïES")
    
    project_memory.store_decision(
        project_id=project_id,
        decision_key="architecture_proposal",
        decision="Arquitetura aprovada pelo time de Data Engineering",
        rationale="Consolida√ß√£o de 3 agentes especializados com valida√ß√£o anti-alucina√ß√£o",
        alternatives=["Arquitetura batch pura", "Arquitetura streaming pura", "H√≠brida"]
    )
    
    print("‚úì Decis√£o de arquitetura armazenada na mem√≥ria do projeto")
    
    # ========== CONTEXTO FINAL ==========
    print_header("FASE 5: CONTEXTO DO PROJETO")
    
    context = project_memory.format_context_for_prompt(project_id)
    print(context)
    
    # ========== RESUMO ==========
    print_header("RESUMO DA DEMONSTRA√á√ÉO")
    
    print("""
    ‚úì Sistema de conhecimento em 3 camadas funcionando
    ‚úì Time de agentes com LLMs diferentes
    ‚úì Agente mestre consolidando respostas
    ‚úì Valida√ß√£o anti-alucina√ß√£o ativa
    ‚úì Mem√≥ria de projeto persistente
    
    O sistema demonstrou:
    1. Diversidade de perspectivas (3 LLMs diferentes)
    2. Consolida√ß√£o inteligente pelo agente mestre
    3. Uso de knowledge base para fundamenta√ß√£o
    4. Persist√™ncia de decis√µes para consist√™ncia futura
    """)
    
    print("=" * 70)
    print("  DEMONSTRA√á√ÉO CONCLU√çDA COM SUCESSO")
    print("=" * 70 + "\n")


if __name__ == "__main__":
    run_demo()
